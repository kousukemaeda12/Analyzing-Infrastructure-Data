# Analyzing-Infrastructure-Data

Using Docker for development work means that applications run in containers and are therefore isolated from other system resources. This is useful in many applications. Here, we use this isolation to take a look at resource consumption for a very simple application.

This experiment begins with a series of simple questions:

   1. What size dataset is too large for a t2.micro to load into memory?
   2. What size dataset is so large that, on a t2.micro, it will prevent Jupyter from fitting different kinds of simple machine learning classification models e.g. a K Nearest Neighbor model? a Decision Tree model? a Logistic Regression?

### Configuration

We will be working with a Jupyter Notebook server being managed by the Docker daemon on a t2.micro running on AWS. We will use the jupyter/datascience-notebook image to run the server.

### Capturing System Usage

While conducting the experiment, we will capture resource consumption using the docker stats tool.
This command will be used to actively monitor system resource usage.

$ docker stats \
  --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}" 
  
We will capture the output to a file(stats-experiment-1.txt, stats-experiment-2.txt) that we can analyze the performance later.

### Experimental Datasets

We will be creating a series of datasets with a specified ùëõ√óùëù size, where ùëõ is the number of samples and ùëù is the number of features. To create these datasets we will use the make_classification function from the sklearn.datasets module.

dataset 	Shape of Feature Set
1 	      100√ó20
2 	      193√ó32
3 	      373√ó54
4 	      720√ó88
5 	      1389√ó144
6 	      2683√ó236
7 	      5179√ó386
8 	      10000√ó632
9 	      19307√ó1036
10 	      37276√ó1697
11 	      71967√ó2779
12 	      138950√ó4552
13 	      268370√ó7455
14 	      517947√ó12211
15 	      1000000√ó20000

### Experimental Process

We will use the following processes to conduct the experiments:

**Experiment 1**
1. Start docker stats capture to stats-experiment-1.txt
2. Create each dataset
3. Pickle each dataset
4. Remove dataset from memory
5. Restart the Jupyter Kernel

**Experiment 2**

1. Start docker stats capture to stats-experiment-2.txt
1. Load each pickled dataset
1. Fit and score a KNN model
1. Restart the Jupyter Kernel
